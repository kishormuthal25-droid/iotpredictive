{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT Anomaly Detection - Data Exploration\n",
    "\n",
    "This notebook explores the SMAP (Soil Moisture Active Passive) and MSL (Mars Science Laboratory) datasets for anomaly detection.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and understand the data structure\n",
    "2. Analyze statistical properties\n",
    "3. Visualize time series patterns\n",
    "4. Identify anomaly characteristics\n",
    "5. Explore sensor correlations\n",
    "6. Prepare insights for model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Project modules\n",
    "from src.data_ingestion.data_loader import DataLoader\n",
    "from src.preprocessing.data_preprocessor import DataPreprocessor\n",
    "from config.settings import Config\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config()\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(config)\n",
    "\n",
    "# Define data paths\n",
    "SMAP_PATH = os.path.join(config.paths['smap_data'])\n",
    "MSL_PATH = os.path.join(config.paths['msl_data'])\n",
    "\n",
    "print(f\"SMAP Data Path: {SMAP_PATH}\")\n",
    "print(f\"MSL Data Path: {MSL_PATH}\")\n",
    "print(f\"\\nConfiguration loaded from: {config.config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SMAP data\n",
    "print(\"Loading SMAP dataset...\")\n",
    "smap_train, smap_test, smap_labels = data_loader.load_smap_data()\n",
    "\n",
    "print(f\"\\nSMAP Dataset Shape:\")\n",
    "print(f\"  Training: {smap_train.shape if smap_train is not None else 'No data'}\")\n",
    "print(f\"  Testing: {smap_test.shape if smap_test is not None else 'No data'}\")\n",
    "print(f\"  Labels: {smap_labels.shape if smap_labels is not None else 'No labels'}\")\n",
    "\n",
    "# Load MSL data\n",
    "print(\"\\nLoading MSL dataset...\")\n",
    "msl_train, msl_test, msl_labels = data_loader.load_msl_data()\n",
    "\n",
    "print(f\"\\nMSL Dataset Shape:\")\n",
    "print(f\"  Training: {msl_train.shape if msl_train is not None else 'No data'}\")\n",
    "print(f\"  Testing: {msl_test.shape if msl_test is not None else 'No data'}\")\n",
    "print(f\"  Labels: {msl_labels.shape if msl_labels is not None else 'No labels'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(data, name):\n",
    "    \"\"\"Calculate basic statistics for the dataset\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    if len(data.shape) == 3:\n",
    "        # Reshape 3D to 2D (samples * timesteps, features)\n",
    "        data_2d = data.reshape(-1, data.shape[-1])\n",
    "    else:\n",
    "        data_2d = data\n",
    "    \n",
    "    df = pd.DataFrame(data_2d)\n",
    "    \n",
    "    stats = {\n",
    "        'Dataset': name,\n",
    "        'Shape': data.shape,\n",
    "        'Total Points': np.prod(data.shape),\n",
    "        'Memory (MB)': data.nbytes / (1024 * 1024),\n",
    "        'Data Type': data.dtype,\n",
    "        'Missing Values': np.isnan(data).sum(),\n",
    "        'Missing %': (np.isnan(data).sum() / np.prod(data.shape)) * 100,\n",
    "        'Infinity Values': np.isinf(data).sum(),\n",
    "    }\n",
    "    \n",
    "    # Statistical measures\n",
    "    stats_df = pd.DataFrame({\n",
    "        'Mean': df.mean(),\n",
    "        'Std': df.std(),\n",
    "        'Min': df.min(),\n",
    "        '25%': df.quantile(0.25),\n",
    "        'Median': df.median(),\n",
    "        '75%': df.quantile(0.75),\n",
    "        'Max': df.max(),\n",
    "        'Skewness': df.skew(),\n",
    "        'Kurtosis': df.kurtosis()\n",
    "    })\n",
    "    \n",
    "    return stats, stats_df\n",
    "\n",
    "# Calculate statistics for all datasets\n",
    "datasets = [\n",
    "    (smap_train, 'SMAP Train'),\n",
    "    (smap_test, 'SMAP Test'),\n",
    "    (msl_train, 'MSL Train'),\n",
    "    (msl_test, 'MSL Test')\n",
    "]\n",
    "\n",
    "all_stats = []\n",
    "for data, name in datasets:\n",
    "    stats, stats_df = calculate_statistics(data, name)\n",
    "    if stats:\n",
    "        all_stats.append(stats)\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Statistics for {name}:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        for key, value in stats.items():\n",
    "            if key != 'Dataset':\n",
    "                print(f\"{key:20s}: {value}\")\n",
    "        print(f\"\\nFeature Statistics:\")\n",
    "        print(stats_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_sample(data, labels=None, name=\"Dataset\", n_samples=3, n_features=5):\n",
    "    \"\"\"Plot sample time series from the dataset\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    n_samples = min(n_samples, len(data))\n",
    "    sample_indices = np.random.choice(len(data), n_samples, replace=False)\n",
    "    \n",
    "    # Select features to plot\n",
    "    n_features = min(n_features, data.shape[-1])\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, n_features, figsize=(20, 3*n_samples))\n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    if n_features == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        sample = data[idx]\n",
    "        \n",
    "        for j in range(n_features):\n",
    "            axes[i, j].plot(sample[:, j], linewidth=0.8)\n",
    "            \n",
    "            # Highlight anomalies if labels are provided\n",
    "            if labels is not None and idx < len(labels):\n",
    "                anomaly_mask = labels[idx].astype(bool)\n",
    "                if anomaly_mask.any():\n",
    "                    axes[i, j].scatter(np.where(anomaly_mask)[0], \n",
    "                                     sample[anomaly_mask, j],\n",
    "                                     color='red', s=10, alpha=0.6,\n",
    "                                     label='Anomaly')\n",
    "            \n",
    "            axes[i, j].set_title(f'Sample {idx} - Feature {j}')\n",
    "            axes[i, j].grid(True, alpha=0.3)\n",
    "            \n",
    "            if j == 0:\n",
    "                axes[i, j].set_ylabel('Value')\n",
    "            if i == n_samples - 1:\n",
    "                axes[i, j].set_xlabel('Time Step')\n",
    "    \n",
    "    plt.suptitle(f'{name} - Time Series Samples', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot samples from each dataset\n",
    "plot_time_series_sample(smap_train, None, \"SMAP Training\", n_samples=2, n_features=4)\n",
    "plot_time_series_sample(smap_test, smap_labels, \"SMAP Test (with anomalies)\", n_samples=2, n_features=4)\n",
    "plot_time_series_sample(msl_train, None, \"MSL Training\", n_samples=2, n_features=4)\n",
    "plot_time_series_sample(msl_test, msl_labels, \"MSL Test (with anomalies)\", n_samples=2, n_features=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Time Series Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_plot(data, labels=None, name=\"Dataset\", sample_idx=0):\n",
    "    \"\"\"Create interactive Plotly visualization\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    sample = data[sample_idx]\n",
    "    n_features = sample.shape[1]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=n_features, cols=1,\n",
    "        subplot_titles=[f'Feature {i}' for i in range(n_features)],\n",
    "        vertical_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        # Add time series\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=list(range(len(sample))),\n",
    "                      y=sample[:, i],\n",
    "                      mode='lines',\n",
    "                      name=f'Feature {i}',\n",
    "                      line=dict(width=1)),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add anomalies if available\n",
    "        if labels is not None and sample_idx < len(labels):\n",
    "            anomaly_mask = labels[sample_idx].astype(bool)\n",
    "            if anomaly_mask.any():\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=np.where(anomaly_mask)[0],\n",
    "                              y=sample[anomaly_mask, i],\n",
    "                              mode='markers',\n",
    "                              name=f'Anomaly F{i}',\n",
    "                              marker=dict(color='red', size=5)),\n",
    "                    row=i+1, col=1\n",
    "                )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=200*n_features,\n",
    "        title_text=f\"{name} - Sample {sample_idx} Interactive View\",\n",
    "        showlegend=False,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Create interactive plots\n",
    "if smap_test is not None:\n",
    "    create_interactive_plot(smap_test, smap_labels, \"SMAP Test\", sample_idx=0)\n",
    "if msl_test is not None:\n",
    "    create_interactive_plot(msl_test, msl_labels, \"MSL Test\", sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(data, name=\"Dataset\", n_features=8):\n",
    "    \"\"\"Plot distribution of values for each feature\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    # Reshape data to 2D\n",
    "    if len(data.shape) == 3:\n",
    "        data_2d = data.reshape(-1, data.shape[-1])\n",
    "    else:\n",
    "        data_2d = data\n",
    "    \n",
    "    n_features = min(n_features, data_2d.shape[1])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_features//2, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        feature_data = data_2d[:, i]\n",
    "        \n",
    "        # Remove NaN values\n",
    "        feature_data = feature_data[~np.isnan(feature_data)]\n",
    "        \n",
    "        # Plot histogram with KDE\n",
    "        axes[i].hist(feature_data, bins=50, alpha=0.7, density=True, edgecolor='black')\n",
    "        \n",
    "        # Fit and plot normal distribution\n",
    "        mu, std = stats.norm.fit(feature_data)\n",
    "        xmin, xmax = axes[i].get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = stats.norm.pdf(x, mu, std)\n",
    "        axes[i].plot(x, p, 'r-', linewidth=2, label=f'Normal\\nŒº={mu:.2f}\\nœÉ={std:.2f}')\n",
    "        \n",
    "        axes[i].set_title(f'Feature {i} Distribution')\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].legend(fontsize=8)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{name} - Feature Distributions', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions\n",
    "plot_distributions(smap_train, \"SMAP Training\")\n",
    "plot_distributions(msl_train, \"MSL Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Anomaly Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_anomalies(test_data, labels, name=\"Dataset\"):\n",
    "    \"\"\"Analyze anomaly patterns in the dataset\"\"\"\n",
    "    if test_data is None or labels is None:\n",
    "        print(f\"No data or labels available for {name}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate anomaly statistics\n",
    "    total_points = np.prod(labels.shape)\n",
    "    anomaly_points = labels.sum()\n",
    "    anomaly_percentage = (anomaly_points / total_points) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Anomaly Analysis for {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total data points: {total_points:,}\")\n",
    "    print(f\"Anomaly points: {anomaly_points:,}\")\n",
    "    print(f\"Anomaly percentage: {anomaly_percentage:.2f}%\")\n",
    "    \n",
    "    # Analyze anomaly lengths\n",
    "    anomaly_lengths = []\n",
    "    for i in range(len(labels)):\n",
    "        anomaly_mask = labels[i].astype(bool)\n",
    "        if anomaly_mask.any():\n",
    "            # Find consecutive anomalies\n",
    "            diff = np.diff(np.concatenate(([0], anomaly_mask.astype(int), [0])))\n",
    "            starts = np.where(diff == 1)[0]\n",
    "            ends = np.where(diff == -1)[0]\n",
    "            lengths = ends - starts\n",
    "            anomaly_lengths.extend(lengths)\n",
    "    \n",
    "    if anomaly_lengths:\n",
    "        print(f\"\\nAnomaly Sequence Statistics:\")\n",
    "        print(f\"  Number of sequences: {len(anomaly_lengths)}\")\n",
    "        print(f\"  Mean length: {np.mean(anomaly_lengths):.2f} time steps\")\n",
    "        print(f\"  Median length: {np.median(anomaly_lengths):.0f} time steps\")\n",
    "        print(f\"  Min length: {np.min(anomaly_lengths)} time steps\")\n",
    "        print(f\"  Max length: {np.max(anomaly_lengths)} time steps\")\n",
    "        \n",
    "        # Plot anomaly length distribution\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(anomaly_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Anomaly Length (time steps)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{name} - Anomaly Length Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(anomaly_lengths)\n",
    "        plt.ylabel('Anomaly Length (time steps)')\n",
    "        plt.title(f'{name} - Anomaly Length Boxplot')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return anomaly_lengths\n",
    "\n",
    "# Analyze anomalies\n",
    "smap_anomaly_lengths = analyze_anomalies(smap_test, smap_labels, \"SMAP\")\n",
    "msl_anomaly_lengths = analyze_anomalies(msl_test, msl_labels, \"MSL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(data, name=\"Dataset\", max_features=20):\n",
    "    \"\"\"Plot correlation matrix between features\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    # Reshape to 2D if necessary\n",
    "    if len(data.shape) == 3:\n",
    "        data_2d = data.reshape(-1, data.shape[-1])\n",
    "    else:\n",
    "        data_2d = data\n",
    "    \n",
    "    # Limit features for visualization\n",
    "    n_features = min(max_features, data_2d.shape[1])\n",
    "    data_subset = data_2d[:, :n_features]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(data_subset.T)\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, \n",
    "                annot=n_features <= 10, \n",
    "                fmt='.2f',\n",
    "                cmap='coolwarm',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "    \n",
    "    plt.title(f'{name} - Feature Correlation Matrix', fontsize=16)\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Feature Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    high_corr_pairs = []\n",
    "    for i in range(n_features):\n",
    "        for j in range(i+1, n_features):\n",
    "            if abs(corr_matrix[i, j]) > 0.8:\n",
    "                high_corr_pairs.append((i, j, corr_matrix[i, j]))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nHighly correlated feature pairs (|correlation| > 0.8):\")\n",
    "        for i, j, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
    "            print(f\"  Feature {i} - Feature {j}: {corr:.3f}\")\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "# Plot correlation matrices\n",
    "smap_corr = plot_correlation_matrix(smap_train, \"SMAP Training\")\n",
    "msl_corr = plot_correlation_matrix(msl_train, \"MSL Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Time Series Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_time_series(data, name=\"Dataset\", sample_idx=0, feature_idx=0):\n",
    "    \"\"\"Decompose time series into trend, seasonal, and residual components\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    # Get single time series\n",
    "    ts = data[sample_idx][:, feature_idx]\n",
    "    \n",
    "    # Create pandas series with datetime index\n",
    "    dates = pd.date_range(start='2023-01-01', periods=len(ts), freq='H')\n",
    "    ts_series = pd.Series(ts, index=dates)\n",
    "    \n",
    "    try:\n",
    "        # Perform decomposition\n",
    "        decomposition = seasonal_decompose(ts_series, model='additive', period=24)\n",
    "        \n",
    "        # Plot decomposition\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(15, 10))\n",
    "        \n",
    "        ts_series.plot(ax=axes[0])\n",
    "        axes[0].set_title(f'{name} - Original Time Series (Sample {sample_idx}, Feature {feature_idx})')\n",
    "        axes[0].set_ylabel('Value')\n",
    "        \n",
    "        decomposition.trend.plot(ax=axes[1])\n",
    "        axes[1].set_title('Trend Component')\n",
    "        axes[1].set_ylabel('Trend')\n",
    "        \n",
    "        decomposition.seasonal.plot(ax=axes[2])\n",
    "        axes[2].set_title('Seasonal Component')\n",
    "        axes[2].set_ylabel('Seasonal')\n",
    "        \n",
    "        decomposition.resid.plot(ax=axes[3])\n",
    "        axes[3].set_title('Residual Component')\n",
    "        axes[3].set_ylabel('Residual')\n",
    "        axes[3].set_xlabel('Time')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return decomposition\n",
    "    except Exception as e:\n",
    "        print(f\"Could not decompose time series: {e}\")\n",
    "        return None\n",
    "\n",
    "# Decompose sample time series\n",
    "if smap_train is not None and len(smap_train) > 0:\n",
    "    smap_decomp = decompose_time_series(smap_train, \"SMAP\", sample_idx=0, feature_idx=0)\n",
    "if msl_train is not None and len(msl_train) > 0:\n",
    "    msl_decomp = decompose_time_series(msl_train, \"MSL\", sample_idx=0, feature_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_autocorrelation(data, name=\"Dataset\", sample_idx=0, n_features=4):\n",
    "    \"\"\"Analyze autocorrelation and partial autocorrelation\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    n_features = min(n_features, data.shape[-1])\n",
    "    \n",
    "    fig, axes = plt.subplots(n_features, 2, figsize=(15, 3*n_features))\n",
    "    if n_features == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        ts = data[sample_idx][:, i]\n",
    "        \n",
    "        # ACF\n",
    "        plot_acf(ts, lags=40, ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f'Feature {i} - Autocorrelation')\n",
    "        \n",
    "        # PACF\n",
    "        try:\n",
    "            plot_pacf(ts, lags=40, ax=axes[i, 1])\n",
    "            axes[i, 1].set_title(f'Feature {i} - Partial Autocorrelation')\n",
    "        except:\n",
    "            axes[i, 1].text(0.5, 0.5, 'Could not compute PACF', \n",
    "                           ha='center', va='center')\n",
    "    \n",
    "    plt.suptitle(f'{name} - ACF and PACF Analysis (Sample {sample_idx})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze autocorrelation\n",
    "if smap_train is not None and len(smap_train) > 0:\n",
    "    analyze_autocorrelation(smap_train, \"SMAP\", sample_idx=0)\n",
    "if msl_train is not None and len(msl_train) > 0:\n",
    "    analyze_autocorrelation(msl_train, \"MSL\", sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Stationarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(data, name=\"Dataset\", n_samples=10, n_features=5):\n",
    "    \"\"\"Test stationarity using Augmented Dickey-Fuller test\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    n_samples = min(n_samples, len(data))\n",
    "    n_features = min(n_features, data.shape[-1])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_features):\n",
    "            ts = data[i][:, j]\n",
    "            \n",
    "            # Remove NaN values\n",
    "            ts = ts[~np.isnan(ts)]\n",
    "            \n",
    "            if len(ts) > 10:  # Need sufficient data for test\n",
    "                try:\n",
    "                    adf_result = adfuller(ts, autolag='AIC')\n",
    "                    results.append({\n",
    "                        'Sample': i,\n",
    "                        'Feature': j,\n",
    "                        'ADF Statistic': adf_result[0],\n",
    "                        'p-value': adf_result[1],\n",
    "                        'Critical Value (5%)': adf_result[4]['5%'],\n",
    "                        'Stationary': adf_result[1] < 0.05\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Stationarity Test Results for {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Tested {len(results)} time series\")\n",
    "        print(f\"Stationary series: {results_df['Stationary'].sum()} ({results_df['Stationary'].mean()*100:.1f}%)\")\n",
    "        print(f\"\\nSample results:\")\n",
    "        print(results_df.head(10))\n",
    "        \n",
    "        # Visualize results\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # p-value distribution\n",
    "        axes[0].hist(results_df['p-value'], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[0].axvline(x=0.05, color='red', linestyle='--', label='Significance level (0.05)')\n",
    "        axes[0].set_xlabel('p-value')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title(f'{name} - p-value Distribution')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Stationarity by feature\n",
    "        feature_stats = results_df.groupby('Feature')['Stationary'].mean()\n",
    "        axes[1].bar(feature_stats.index, feature_stats.values, edgecolor='black', alpha=0.7)\n",
    "        axes[1].set_xlabel('Feature Index')\n",
    "        axes[1].set_ylabel('Proportion Stationary')\n",
    "        axes[1].set_title(f'{name} - Stationarity by Feature')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test stationarity\n",
    "smap_stationarity = test_stationarity(smap_train, \"SMAP\")\n",
    "msl_stationarity = test_stationarity(msl_train, \"MSL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Frequency Domain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frequency_domain(data, name=\"Dataset\", sample_idx=0, n_features=4):\n",
    "    \"\"\"Analyze time series in frequency domain using FFT\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    n_features = min(n_features, data.shape[-1])\n",
    "    \n",
    "    fig, axes = plt.subplots(n_features, 2, figsize=(15, 3*n_features))\n",
    "    if n_features == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        ts = data[sample_idx][:, i]\n",
    "        \n",
    "        # Remove NaN values\n",
    "        ts = ts[~np.isnan(ts)]\n",
    "        \n",
    "        # Time domain\n",
    "        axes[i, 0].plot(ts)\n",
    "        axes[i, 0].set_title(f'Feature {i} - Time Domain')\n",
    "        axes[i, 0].set_xlabel('Time Step')\n",
    "        axes[i, 0].set_ylabel('Value')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Frequency domain\n",
    "        n = len(ts)\n",
    "        yf = fft(ts)\n",
    "        xf = fftfreq(n, 1)[:n//2]\n",
    "        \n",
    "        axes[i, 1].plot(xf, 2.0/n * np.abs(yf[:n//2]))\n",
    "        axes[i, 1].set_title(f'Feature {i} - Frequency Domain')\n",
    "        axes[i, 1].set_xlabel('Frequency')\n",
    "        axes[i, 1].set_ylabel('Amplitude')\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Mark dominant frequencies\n",
    "        power = np.abs(yf[:n//2])\n",
    "        peaks, _ = find_peaks(power, height=np.percentile(power, 90))\n",
    "        if len(peaks) > 0:\n",
    "            axes[i, 1].plot(xf[peaks], 2.0/n * power[peaks], 'ro', markersize=5)\n",
    "    \n",
    "    plt.suptitle(f'{name} - Frequency Domain Analysis (Sample {sample_idx})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze frequency domain\n",
    "if smap_train is not None and len(smap_train) > 0:\n",
    "    analyze_frequency_domain(smap_train, \"SMAP\", sample_idx=0)\n",
    "if msl_train is not None and len(msl_train) > 0:\n",
    "    analyze_frequency_domain(msl_train, \"MSL\", sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dimensionality_reduction(data, labels=None, name=\"Dataset\", n_samples=1000):\n",
    "    \"\"\"Perform PCA and t-SNE for visualization\"\"\"\n",
    "    if data is None:\n",
    "        print(f\"No data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    # Reshape to 2D\n",
    "    if len(data.shape) == 3:\n",
    "        data_2d = data.reshape(data.shape[0], -1)\n",
    "    else:\n",
    "        data_2d = data\n",
    "    \n",
    "    # Sample data if too large\n",
    "    if len(data_2d) > n_samples:\n",
    "        indices = np.random.choice(len(data_2d), n_samples, replace=False)\n",
    "        data_sample = data_2d[indices]\n",
    "        labels_sample = labels[indices] if labels is not None else None\n",
    "    else:\n",
    "        data_sample = data_2d\n",
    "        labels_sample = labels\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_sample)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=min(50, data_scaled.shape[1]))\n",
    "    pca_result = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    # t-SNE (on PCA result for speed)\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    tsne_result = tsne.fit_transform(pca_result[:, :10])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # PCA variance explained\n",
    "    axes[0, 0].plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    axes[0, 0].set_xlabel('Number of Components')\n",
    "    axes[0, 0].set_ylabel('Cumulative Explained Variance')\n",
    "    axes[0, 0].set_title('PCA - Variance Explained')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # PCA 2D projection\n",
    "    scatter1 = axes[0, 1].scatter(pca_result[:, 0], pca_result[:, 1], \n",
    "                                  c=labels_sample.mean(axis=1) if labels_sample is not None else None,\n",
    "                                  cmap='coolwarm', alpha=0.6, s=10)\n",
    "    axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    axes[0, 1].set_title('PCA - 2D Projection')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    if labels_sample is not None:\n",
    "        plt.colorbar(scatter1, ax=axes[0, 1], label='Anomaly Score')\n",
    "    \n",
    "    # t-SNE projection\n",
    "    scatter2 = axes[1, 0].scatter(tsne_result[:, 0], tsne_result[:, 1],\n",
    "                                  c=labels_sample.mean(axis=1) if labels_sample is not None else None,\n",
    "                                  cmap='coolwarm', alpha=0.6, s=10)\n",
    "    axes[1, 0].set_xlabel('t-SNE 1')\n",
    "    axes[1, 0].set_ylabel('t-SNE 2')\n",
    "    axes[1, 0].set_title('t-SNE - 2D Projection')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    if labels_sample is not None:\n",
    "        plt.colorbar(scatter2, ax=axes[1, 0], label='Anomaly Score')\n",
    "    \n",
    "    # Feature importance (based on PCA components)\n",
    "    feature_importance = np.abs(pca.components_[0])\n",
    "    top_features = np.argsort(feature_importance)[-20:]\n",
    "    axes[1, 1].barh(range(len(top_features)), feature_importance[top_features])\n",
    "    axes[1, 1].set_xlabel('Absolute Loading')\n",
    "    axes[1, 1].set_ylabel('Feature Rank')\n",
    "    axes[1, 1].set_title('Top 20 Features - PC1 Loadings')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{name} - Dimensionality Reduction Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pca, tsne_result\n",
    "\n",
    "# Perform dimensionality reduction\n",
    "if smap_test is not None:\n",
    "    smap_pca, smap_tsne = perform_dimensionality_reduction(smap_test, smap_labels, \"SMAP\")\n",
    "if msl_test is not None:\n",
    "    msl_pca, msl_tsne = perform_dimensionality_reduction(msl_test, msl_labels, \"MSL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary():\n",
    "    \"\"\"Generate summary of key findings\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA EXPLORATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìä DATASET CHARACTERISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    datasets_info = [\n",
    "        (\"SMAP Train\", smap_train),\n",
    "        (\"SMAP Test\", smap_test),\n",
    "        (\"MSL Train\", msl_train),\n",
    "        (\"MSL Test\", msl_test)\n",
    "    ]\n",
    "    \n",
    "    for name, data in datasets_info:\n",
    "        if data is not None:\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  ‚Ä¢ Shape: {data.shape}\")\n",
    "            print(f\"  ‚Ä¢ Memory: {data.nbytes / (1024**2):.2f} MB\")\n",
    "            print(f\"  ‚Ä¢ Value range: [{np.min(data):.3f}, {np.max(data):.3f}]\")\n",
    "            print(f\"  ‚Ä¢ Missing values: {np.isnan(data).sum()} ({np.isnan(data).mean()*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nüîç KEY OBSERVATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    observations = [\n",
    "        \"1. Time Series Properties:\",\n",
    "        \"   ‚Ä¢ Both datasets contain multivariate time series from IoT sensors\",\n",
    "        \"   ‚Ä¢ Data shows complex temporal patterns with varying frequencies\",\n",
    "        \"   ‚Ä¢ Some features exhibit strong autocorrelation\",\n",
    "        \"\",\n",
    "        \"2. Anomaly Characteristics:\",\n",
    "        \"   ‚Ä¢ Anomalies appear as both point outliers and sequential patterns\",\n",
    "        \"   ‚Ä¢ Anomaly duration varies significantly (from single points to extended sequences)\",\n",
    "        \"   ‚Ä¢ Different features may have correlated anomalies\",\n",
    "        \"\",\n",
    "        \"3. Statistical Properties:\",\n",
    "        \"   ‚Ä¢ Features have different scales and distributions\",\n",
    "        \"   ‚Ä¢ Some features show non-stationary behavior\",\n",
    "        \"   ‚Ä¢ Correlation exists between certain sensor readings\",\n",
    "        \"\",\n",
    "        \"4. Data Quality:\",\n",
    "        \"   ‚Ä¢ Generally clean data with minimal missing values\",\n",
    "        \"   ‚Ä¢ Some features may benefit from normalization\",\n",
    "        \"   ‚Ä¢ Outliers present in normal operation (not just anomalies)\"\n",
    "    ]\n",
    "    \n",
    "    for obs in observations:\n",
    "        print(obs)\n",
    "    \n",
    "    print(\"\\nüí° RECOMMENDATIONS FOR MODELING:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    recommendations = [\n",
    "        \"1. Preprocessing:\",\n",
    "        \"   ‚úì Apply normalization (MinMax or Standard scaling)\",\n",
    "        \"   ‚úì Handle missing values with interpolation\",\n",
    "        \"   ‚úì Consider feature engineering (rolling statistics, FFT features)\",\n",
    "        \"\",\n",
    "        \"2. Model Selection:\",\n",
    "        \"   ‚úì LSTM-based models suitable for capturing temporal dependencies\",\n",
    "        \"   ‚úì Autoencoder architectures for reconstruction-based detection\",\n",
    "        \"   ‚úì Consider ensemble approaches for robustness\",\n",
    "        \"\",\n",
    "        \"3. Training Strategy:\",\n",
    "        \"   ‚úì Use sliding windows for sequence generation\",\n",
    "        \"   ‚úì Implement proper train/validation split\",\n",
    "        \"   ‚úì Monitor for overfitting on normal patterns\",\n",
    "        \"\",\n",
    "        \"4. Evaluation:\",\n",
    "        \"   ‚úì Focus on precision-recall trade-offs\",\n",
    "        \"   ‚úì Consider point-wise and range-based metrics\",\n",
    "        \"   ‚úì Validate on both datasets for generalization\"\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"END OF EXPLORATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Generate summary\n",
    "generate_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Export Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key statistics and findings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "exploration_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'datasets': {},\n",
    "    'anomaly_analysis': {},\n",
    "    'recommendations': []\n",
    "}\n",
    "\n",
    "# Collect dataset info\n",
    "for name, data, labels in [('smap_train', smap_train, None),\n",
    "                           ('smap_test', smap_test, smap_labels),\n",
    "                           ('msl_train', msl_train, None),\n",
    "                           ('msl_test', msl_test, msl_labels)]:\n",
    "    if data is not None:\n",
    "        exploration_results['datasets'][name] = {\n",
    "            'shape': data.shape,\n",
    "            'memory_mb': float(data.nbytes / (1024**2)),\n",
    "            'min_value': float(np.min(data)),\n",
    "            'max_value': float(np.max(data)),\n",
    "            'mean_value': float(np.mean(data)),\n",
    "            'std_value': float(np.std(data)),\n",
    "            'missing_count': int(np.isnan(data).sum()),\n",
    "            'has_anomalies': labels is not None\n",
    "        }\n",
    "\n",
    "# Save to JSON\n",
    "output_path = '../data/processed/data_exploration_results.json'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(exploration_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Exploration results saved to: {output_path}\")\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"   1. Run model_experiments.ipynb for model development\")\n",
    "print(\"   2. Use visualization.ipynb for detailed result analysis\")\n",
    "print(\"   3. Deploy models using the main application scripts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}