{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT Anomaly Detection - Model Experiments\n",
    "\n",
    "This notebook experiments with various anomaly detection models for the IoT predictive maintenance system.\n",
    "\n",
    "## Models to Experiment:\n",
    "1. **LSTM Predictor** - Predicts next values and flags deviations\n",
    "2. **LSTM Autoencoder** - Reconstruction-based anomaly detection\n",
    "3. **LSTM-VAE** - Variational autoencoder for probabilistic detection\n",
    "4. **Transformer Forecaster** - Attention-based time series prediction\n",
    "\n",
    "## Objectives:\n",
    "- Train and evaluate different model architectures\n",
    "- Compare performance metrics\n",
    "- Optimize hyperparameters\n",
    "- Select best models for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Project modules\n",
    "from src.data_ingestion.data_loader import DataLoader\n",
    "from src.preprocessing.data_preprocessor import DataPreprocessor\n",
    "from src.preprocessing.feature_engineering import FeatureEngineer\n",
    "from src.anomaly_detection.lstm_detector import LSTMDetector\n",
    "from src.anomaly_detection.lstm_autoencoder import LSTMAutoencoder\n",
    "from src.anomaly_detection.lstm_vae import LSTMVAE\n",
    "from src.anomaly_detection.model_evaluator import ModelEvaluator\n",
    "from src.forecasting.transformer_forecaster import TransformerForecaster\n",
    "from src.forecasting.lstm_forecaster import LSTMForecaster\n",
    "from config.settings import Config\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"\\nLibraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config()\n",
    "\n",
    "# Initialize components\n",
    "data_loader = DataLoader(config)\n",
    "preprocessor = DataPreprocessor(config)\n",
    "feature_engineer = FeatureEngineer(config)\n",
    "evaluator = ModelEvaluator(config)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Experiment settings:\")\n",
    "print(f\"  - Sequence length: {config.anomaly_detection['general']['sequence_length']}\")\n",
    "print(f\"  - Prediction length: {config.anomaly_detection['general']['prediction_length']}\")\n",
    "print(f\"  - Contamination ratio: {config.anomaly_detection['general']['contamination']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load SMAP data\n",
    "smap_train, smap_test, smap_labels = data_loader.load_smap_data()\n",
    "print(f\"SMAP - Train: {smap_train.shape if smap_train is not None else 'None'}, \"\n",
    "      f\"Test: {smap_test.shape if smap_test is not None else 'None'}\")\n",
    "\n",
    "# Load MSL data\n",
    "msl_train, msl_test, msl_labels = data_loader.load_msl_data()\n",
    "print(f\"MSL - Train: {msl_train.shape if msl_train is not None else 'None'}, \"\n",
    "      f\"Test: {msl_test.shape if msl_test is not None else 'None'}\")\n",
    "\n",
    "# Select dataset for experiments (you can change this)\n",
    "DATASET = 'SMAP'  # Change to 'MSL' to experiment with MSL data\n",
    "\n",
    "if DATASET == 'SMAP':\n",
    "    X_train, X_test, y_test = smap_train, smap_test, smap_labels\n",
    "else:\n",
    "    X_train, X_test, y_test = msl_train, msl_test, msl_labels\n",
    "\n",
    "print(f\"\\nUsing {DATASET} dataset for experiments\")\n",
    "print(f\"Training samples: {len(X_train) if X_train is not None else 0}\")\n",
    "print(f\"Test samples: {len(X_test) if X_test is not None else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test, config):\n",
    "    \"\"\"Preprocess the data for model training\"\"\"\n",
    "    \n",
    "    # Reshape if needed (flatten time series)\n",
    "    if len(X_train.shape) == 3:\n",
    "        n_samples, n_timesteps, n_features = X_train.shape\n",
    "        X_train_2d = X_train.reshape(-1, n_features)\n",
    "        X_test_2d = X_test.reshape(-1, n_features)\n",
    "    else:\n",
    "        X_train_2d = X_train\n",
    "        X_test_2d = X_test\n",
    "        n_features = X_train.shape[1]\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_scaled = scaler.fit_transform(X_train_2d)\n",
    "    X_test_scaled = scaler.transform(X_test_2d)\n",
    "    \n",
    "    # Reshape back to 3D if needed\n",
    "    if len(X_train.shape) == 3:\n",
    "        X_train_scaled = X_train_scaled.reshape(n_samples, n_timesteps, n_features)\n",
    "        X_test_scaled = X_test_scaled.reshape(X_test.shape[0], n_timesteps, n_features)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "# Preprocess data\n",
    "if X_train is not None and X_test is not None:\n",
    "    X_train_scaled, X_test_scaled, scaler = preprocess_data(X_train, X_test, config)\n",
    "    print(f\"Data preprocessed successfully!\")\n",
    "    print(f\"Scaled train shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Scaled test shape: {X_test_scaled.shape}\")\n",
    "else:\n",
    "    print(\"No data available for preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for time series models\n",
    "def create_sequences(data, seq_length, stride=1):\n",
    "    \"\"\"Create overlapping sequences from time series data\"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    if len(data.shape) == 3:\n",
    "        # Already in sequence format\n",
    "        return data\n",
    "    \n",
    "    for i in range(0, len(data) - seq_length + 1, stride):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "    \n",
    "    return np.array(sequences)\n",
    "\n",
    "# Create sequences if data is 2D\n",
    "if X_train_scaled is not None and len(X_train_scaled.shape) == 2:\n",
    "    sequence_length = config.anomaly_detection['general']['sequence_length']\n",
    "    \n",
    "    X_train_seq = create_sequences(X_train_scaled, sequence_length, stride=10)\n",
    "    X_test_seq = create_sequences(X_test_scaled, sequence_length, stride=1)\n",
    "    \n",
    "    # Adjust labels for sequences\n",
    "    if y_test is not None:\n",
    "        y_test_seq = create_sequences(y_test, sequence_length, stride=1)\n",
    "        y_test_seq = (y_test_seq.sum(axis=1) > 0).astype(int)  # Any anomaly in sequence\n",
    "    else:\n",
    "        y_test_seq = None\n",
    "    \n",
    "    print(f\"\\nSequences created:\")\n",
    "    print(f\"Train sequences: {X_train_seq.shape}\")\n",
    "    print(f\"Test sequences: {X_test_seq.shape}\")\n",
    "else:\n",
    "    X_train_seq = X_train_scaled\n",
    "    X_test_seq = X_test_scaled\n",
    "    y_test_seq = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name, patience=10):\n",
    "    \"\"\"Get standard callbacks for model training\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    checkpoint_dir = f\"../data/models/{model_name}\"\n",
    "    log_dir = f\"../logs/tensorboard/{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    callbacks_list = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(checkpoint_dir, 'best_model.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[0].set_title(f'{model_name} - Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Additional metrics if available\n",
    "    metrics = [k for k in history.history.keys() if k not in ['loss', 'val_loss']]\n",
    "    if metrics:\n",
    "        for metric in metrics[:1]:  # Plot first additional metric\n",
    "            axes[1].plot(history.history[metric], label=f'Train {metric}')\n",
    "            if f'val_{metric}' in history.history:\n",
    "                axes[1].plot(history.history[f'val_{metric}'], label=f'Val {metric}')\n",
    "        axes[1].set_title(f'{model_name} - Metrics')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Metric Value')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 1: LSTM Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_predictor(input_shape, config):\n",
    "    \"\"\"Build LSTM predictor model\"\"\"\n",
    "    \n",
    "    lstm_config = config.anomaly_detection['lstm_predictor']\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        layers.LSTM(lstm_config['architecture']['layers'][0], \n",
    "                   return_sequences=True,\n",
    "                   input_shape=input_shape),\n",
    "        layers.Dropout(lstm_config['architecture']['dropout']),\n",
    "        \n",
    "        layers.LSTM(lstm_config['architecture']['layers'][1],\n",
    "                   return_sequences=True),\n",
    "        layers.Dropout(lstm_config['architecture']['dropout']),\n",
    "        \n",
    "        layers.LSTM(lstm_config['architecture']['layers'][2],\n",
    "                   return_sequences=False),\n",
    "        layers.Dropout(lstm_config['architecture']['dropout']),\n",
    "        \n",
    "        layers.Dense(input_shape[1], activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=lstm_config['training']['learning_rate']),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train LSTM Predictor\n",
    "if X_train_seq is not None:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training LSTM Predictor\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Prepare data for prediction (predict next timestep)\n",
    "    X_train_pred = X_train_seq[:, :-1, :]\n",
    "    y_train_pred = X_train_seq[:, -1, :]\n",
    "    \n",
    "    X_test_pred = X_test_seq[:, :-1, :]\n",
    "    y_test_pred = X_test_seq[:, -1, :]\n",
    "    \n",
    "    # Build model\n",
    "    lstm_predictor = build_lstm_predictor(\n",
    "        input_shape=(X_train_pred.shape[1], X_train_pred.shape[2]),\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(lstm_predictor.summary())\n",
    "    \n",
    "    # Train model\n",
    "    history_lstm_pred = lstm_predictor.fit(\n",
    "        X_train_pred, y_train_pred,\n",
    "        epochs=config.anomaly_detection['lstm_predictor']['training']['epochs'],\n",
    "        batch_size=config.anomaly_detection['lstm_predictor']['training']['batch_size'],\n",
    "        validation_split=config.anomaly_detection['lstm_predictor']['training']['validation_split'],\n",
    "        callbacks=get_callbacks('lstm_predictor'),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history_lstm_pred, 'LSTM Predictor')\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    predictions = lstm_predictor.predict(X_test_pred)\n",
    "    mse_errors = np.mean((predictions - y_test_pred) ** 2, axis=1)\n",
    "    \n",
    "    # Determine threshold\n",
    "    threshold_percentile = config.anomaly_detection['lstm_predictor']['threshold']['value']\n",
    "    threshold = np.percentile(mse_errors, threshold_percentile)\n",
    "    \n",
    "    print(f\"\\nPrediction errors - Min: {mse_errors.min():.4f}, Max: {mse_errors.max():.4f}\")\n",
    "    print(f\"Threshold ({threshold_percentile}th percentile): {threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 2: LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_autoencoder(input_shape, config):\n",
    "    \"\"\"Build LSTM Autoencoder model\"\"\"\n",
    "    \n",
    "    ae_config = config.anomaly_detection['lstm_autoencoder']\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_input = layers.Input(shape=input_shape)\n",
    "    x = encoder_input\n",
    "    \n",
    "    for i, units in enumerate(ae_config['architecture']['encoder_layers']):\n",
    "        x = layers.LSTM(units, return_sequences=True)(x)\n",
    "        x = layers.Dropout(ae_config['architecture']['dropout'])(x)\n",
    "    \n",
    "    # Latent representation\n",
    "    x = layers.LSTM(ae_config['architecture']['latent_dim'], return_sequences=False)(x)\n",
    "    encoder_output = layers.RepeatVector(input_shape[0])(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = encoder_output\n",
    "    for units in ae_config['architecture']['decoder_layers']:\n",
    "        x = layers.LSTM(units, return_sequences=True)(x)\n",
    "        x = layers.Dropout(ae_config['architecture']['dropout'])(x)\n",
    "    \n",
    "    decoder_output = layers.TimeDistributed(layers.Dense(input_shape[1]))(x)\n",
    "    \n",
    "    # Build model\n",
    "    autoencoder = models.Model(encoder_input, decoder_output)\n",
    "    \n",
    "    autoencoder.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=ae_config['training']['learning_rate']),\n",
    "        loss=ae_config['training']['loss'],\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Train LSTM Autoencoder\n",
    "if X_train_seq is not None:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training LSTM Autoencoder\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Build model\n",
    "    lstm_autoencoder = build_lstm_autoencoder(\n",
    "        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(lstm_autoencoder.summary())\n",
    "    \n",
    "    # Train model\n",
    "    history_lstm_ae = lstm_autoencoder.fit(\n",
    "        X_train_seq, X_train_seq,  # Autoencoder reconstructs input\n",
    "        epochs=config.anomaly_detection['lstm_autoencoder']['training']['epochs'],\n",
    "        batch_size=config.anomaly_detection['lstm_autoencoder']['training']['batch_size'],\n",
    "        validation_split=config.anomaly_detection['lstm_autoencoder']['training']['validation_split'],\n",
    "        callbacks=get_callbacks('lstm_autoencoder'),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history_lstm_ae, 'LSTM Autoencoder')\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    reconstructions = lstm_autoencoder.predict(X_test_seq)\n",
    "    mse_errors_ae = np.mean((reconstructions - X_test_seq) ** 2, axis=(1, 2))\n",
    "    \n",
    "    # Determine threshold\n",
    "    n_std = config.anomaly_detection['lstm_autoencoder']['threshold']['n_std']\n",
    "    threshold_ae = np.mean(mse_errors_ae) + n_std * np.std(mse_errors_ae)\n",
    "    \n",
    "    print(f\"\\nReconstruction errors - Min: {mse_errors_ae.min():.4f}, Max: {mse_errors_ae.max():.4f}\")\n",
    "    print(f\"Threshold (mean + {n_std}*std): {threshold_ae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment 3: LSTM-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_vae(input_shape, config):\n",
    "    \"\"\"Build LSTM-VAE model\"\"\"\n",
    "    \n",
    "    vae_config = config.anomaly_detection['lstm_vae']\n",
    "    latent_dim = vae_config['architecture']['latent_dim']\n",
    "    \n",
    "    # Sampling layer\n",
    "    class Sampling(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_input = layers.Input(shape=input_shape)\n",
    "    x = encoder_input\n",
    "    \n",
    "    for units in vae_config['architecture']['encoder_layers']:\n",
    "        x = layers.LSTM(units, return_sequences=True)(x)\n",
    "        x = layers.Dropout(vae_config['architecture']['dropout'])(x)\n",
    "    \n",
    "    x = layers.LSTM(latent_dim * 2, return_sequences=False)(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = models.Model(encoder_input, [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    latent_input = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.RepeatVector(input_shape[0])(latent_input)\n",
    "    \n",
    "    for units in vae_config['architecture']['decoder_layers']:\n",
    "        x = layers.LSTM(units, return_sequences=True)(x)\n",
    "        x = layers.Dropout(vae_config['architecture']['dropout'])(x)\n",
    "    \n",
    "    decoder_output = layers.TimeDistributed(layers.Dense(input_shape[1]))(x)\n",
    "    \n",
    "    decoder = models.Model(latent_input, decoder_output, name='decoder')\n",
    "    \n",
    "    # VAE Model\n",
    "    class VAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "        \n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                \n",
    "                reconstruction_loss = tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        keras.losses.mse(data, reconstruction), axis=(1, 2)\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "                \n",
    "                total_loss = reconstruction_loss * vae_config['training']['reconstruction_weight'] + \\\n",
    "                           kl_loss * vae_config['training']['kl_weight']\n",
    "            \n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            \n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            \n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "    \n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam(learning_rate=vae_config['training']['learning_rate']))\n",
    "    \n",
    "    return vae\n",
    "\n",
    "# Train LSTM-VAE\n",
    "if X_train_seq is not None:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training LSTM-VAE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Build model\n",
    "    lstm_vae = build_lstm_vae(\n",
    "        input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history_lstm_vae = lstm_vae.fit(\n",
    "        X_train_seq,\n",
    "        epochs=config.anomaly_detection['lstm_vae']['training']['epochs'],\n",
    "        batch_size=config.anomaly_detection['lstm_vae']['training']['batch_size'],\n",
    "        callbacks=get_callbacks('lstm_vae'),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history_lstm_vae, 'LSTM-VAE')\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    z_mean, z_log_var, z = lstm_vae.encoder.predict(X_test_seq)\n",
    "    reconstructions_vae = lstm_vae.decoder.predict(z)\n",
    "    mse_errors_vae = np.mean((reconstructions_vae - X_test_seq) ** 2, axis=(1, 2))\n",
    "    \n",
    "    # Determine threshold\n",
    "    percentile = config.anomaly_detection['lstm_vae']['threshold']['percentile']\n",
    "    threshold_vae = np.percentile(mse_errors_vae, percentile)\n",
    "    \n",
    "    print(f\"\\nReconstruction errors - Min: {mse_errors_vae.min():.4f}, Max: {mse_errors_vae.max():.4f}\")\n",
    "    print(f\"Threshold ({percentile}th percentile): {threshold_vae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(errors, threshold, y_true, model_name):\n",
    "    \"\"\"Evaluate anomaly detection model\"\"\"\n",
    "    \n",
    "    # Predict anomalies\n",
    "    y_pred = (errors > threshold).astype(int)\n",
    "    \n",
    "    # Handle case where y_true might be multi-dimensional\n",
    "    if y_true is not None:\n",
    "        if len(y_true.shape) > 1:\n",
    "            y_true_flat = (y_true.sum(axis=1) > 0).astype(int)\n",
    "        else:\n",
    "            y_true_flat = y_true\n",
    "        \n",
    "        # Ensure same length\n",
    "        min_len = min(len(y_pred), len(y_true_flat))\n",
    "        y_pred = y_pred[:min_len]\n",
    "        y_true_flat = y_true_flat[:min_len]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy_score(y_true_flat, y_pred),\n",
    "            'Precision': precision_score(y_true_flat, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_true_flat, y_pred, zero_division=0),\n",
    "            'F1-Score': f1_score(y_true_flat, y_pred, zero_division=0),\n",
    "            'ROC-AUC': roc_auc_score(y_true_flat, errors[:min_len]) if y_true_flat.sum() > 0 else 0,\n",
    "            'Threshold': threshold,\n",
    "            'Anomalies Detected': y_pred.sum(),\n",
    "            'True Anomalies': y_true_flat.sum()\n",
    "        }\n",
    "        \n",
    "        return metrics, y_pred, y_true_flat\n",
    "    else:\n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Threshold': threshold,\n",
    "            'Anomalies Detected': y_pred.sum(),\n",
    "            'Detection Rate': y_pred.sum() / len(y_pred)\n",
    "        }\n",
    "        return metrics, y_pred, None\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "predictions = {}\n",
    "\n",
    "if 'mse_errors' in locals():\n",
    "    metrics, y_pred, y_true = evaluate_model(mse_errors, threshold, y_test_seq, 'LSTM Predictor')\n",
    "    results.append(metrics)\n",
    "    predictions['LSTM Predictor'] = (y_pred, y_true, mse_errors)\n",
    "\n",
    "if 'mse_errors_ae' in locals():\n",
    "    metrics, y_pred, y_true = evaluate_model(mse_errors_ae, threshold_ae, y_test_seq, 'LSTM Autoencoder')\n",
    "    results.append(metrics)\n",
    "    predictions['LSTM Autoencoder'] = (y_pred, y_true, mse_errors_ae)\n",
    "\n",
    "if 'mse_errors_vae' in locals():\n",
    "    metrics, y_pred, y_true = evaluate_model(mse_errors_vae, threshold_vae, y_test_seq, 'LSTM-VAE')\n",
    "    results.append(metrics)\n",
    "    predictions['LSTM-VAE'] = (y_pred, y_true, mse_errors_vae)\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Model Performance Comparison\")\n",
    "    print(\"=\"*50)\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics comparison\n",
    "if results:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Anomalies Detected']\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        if metric in results_df.columns:\n",
    "            axes[i].bar(results_df['Model'], results_df[metric])\n",
    "            axes[i].set_title(metric)\n",
    "            axes[i].set_ylabel('Value')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for j, v in enumerate(results_df[metric]):\n",
    "                axes[i].text(j, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distributions\n",
    "if predictions:\n",
    "    fig, axes = plt.subplots(1, len(predictions), figsize=(15, 4))\n",
    "    if len(predictions) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (model_name, (y_pred, y_true, errors)) in enumerate(predictions.items()):\n",
    "        axes[i].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[i].axvline(x=np.mean(errors), color='green', linestyle='--', label='Mean')\n",
    "        \n",
    "        # Add threshold line\n",
    "        if model_name == 'LSTM Predictor':\n",
    "            axes[i].axvline(x=threshold, color='red', linestyle='--', label='Threshold')\n",
    "        elif model_name == 'LSTM Autoencoder':\n",
    "            axes[i].axvline(x=threshold_ae, color='red', linestyle='--', label='Threshold')\n",
    "        elif model_name == 'LSTM-VAE':\n",
    "            axes[i].axvline(x=threshold_vae, color='red', linestyle='--', label='Threshold')\n",
    "        \n",
    "        axes[i].set_title(f'{model_name} - Error Distribution')\n",
    "        axes[i].set_xlabel('Reconstruction Error')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "if predictions and y_test_seq is not None:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    for model_name, (y_pred, y_true, errors) in predictions.items():\n",
    "        if y_true is not None and y_true.sum() > 0:\n",
    "            fpr, tpr, _ = roc_curve(y_true, errors[:len(y_true)])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Anomaly Detection on Sample Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_anomaly_detection(X_test, predictions_dict, sample_idx=0, n_features=3):\n",
    "    \"\"\"Visualize anomaly detection on a sample time series\"\"\"\n",
    "    \n",
    "    if not predictions_dict:\n",
    "        print(\"No predictions available\")\n",
    "        return\n",
    "    \n",
    "    n_models = len(predictions_dict)\n",
    "    fig, axes = plt.subplots(n_models + 1, n_features, figsize=(15, 3*(n_models+1)))\n",
    "    \n",
    "    if n_features == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Original time series\n",
    "    for j in range(n_features):\n",
    "        axes[0, j].plot(X_test[sample_idx][:, j], label='Original', color='blue', alpha=0.7)\n",
    "        axes[0, j].set_title(f'Feature {j} - Original')\n",
    "        axes[0, j].grid(True, alpha=0.3)\n",
    "        axes[0, j].legend()\n",
    "    \n",
    "    # Model predictions\n",
    "    for i, (model_name, (y_pred, y_true, errors)) in enumerate(predictions_dict.items(), 1):\n",
    "        # Get anomaly points for this sample\n",
    "        if sample_idx < len(y_pred):\n",
    "            is_anomaly = y_pred[sample_idx]\n",
    "            \n",
    "            for j in range(n_features):\n",
    "                axes[i, j].plot(X_test[sample_idx][:, j], label='Normal', color='blue', alpha=0.7)\n",
    "                \n",
    "                if is_anomaly:\n",
    "                    axes[i, j].fill_between(range(len(X_test[sample_idx])), \n",
    "                                           X_test[sample_idx][:, j].min(),\n",
    "                                           X_test[sample_idx][:, j].max(),\n",
    "                                           color='red', alpha=0.2, label='Anomaly')\n",
    "                \n",
    "                axes[i, j].set_title(f'{model_name} - Feature {j}')\n",
    "                axes[i, j].grid(True, alpha=0.3)\n",
    "                axes[i, j].legend()\n",
    "    \n",
    "    plt.suptitle(f'Anomaly Detection Visualization - Sample {sample_idx}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize anomaly detection\n",
    "if X_test_seq is not None and predictions:\n",
    "    visualize_anomaly_detection(X_test_seq, predictions, sample_idx=5, n_features=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(X_train, X_val, model_type='autoencoder'):\n",
    "    \"\"\"Simple hyperparameter search for model optimization\"\"\"\n",
    "    \n",
    "    param_grid = {\n",
    "        'lstm_units': [32, 64, 128],\n",
    "        'latent_dim': [8, 16, 32],\n",
    "        'dropout': [0.1, 0.2, 0.3],\n",
    "        'learning_rate': [0.001, 0.0001]\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for lstm_units in param_grid['lstm_units']:\n",
    "        for latent_dim in param_grid['latent_dim']:\n",
    "            for dropout in param_grid['dropout']:\n",
    "                for lr in param_grid['learning_rate']:\n",
    "                    \n",
    "                    # Build simple autoencoder with current params\n",
    "                    model = models.Sequential([\n",
    "                        layers.LSTM(lstm_units, return_sequences=True, \n",
    "                                   input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                        layers.Dropout(dropout),\n",
    "                        layers.LSTM(latent_dim, return_sequences=False),\n",
    "                        layers.RepeatVector(X_train.shape[1]),\n",
    "                        layers.LSTM(latent_dim, return_sequences=True),\n",
    "                        layers.Dropout(dropout),\n",
    "                        layers.LSTM(lstm_units, return_sequences=True),\n",
    "                        layers.TimeDistributed(layers.Dense(X_train.shape[2]))\n",
    "                    ])\n",
    "                    \n",
    "                    model.compile(optimizer=optimizers.Adam(learning_rate=lr), \n",
    "                                loss='mse')\n",
    "                    \n",
    "                    # Quick training\n",
    "                    history = model.fit(\n",
    "                        X_train, X_train,\n",
    "                        validation_data=(X_val, X_val),\n",
    "                        epochs=10,\n",
    "                        batch_size=32,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    val_loss = min(history.history['val_loss'])\n",
    "                    \n",
    "                    results.append({\n",
    "                        'lstm_units': lstm_units,\n",
    "                        'latent_dim': latent_dim,\n",
    "                        'dropout': dropout,\n",
    "                        'learning_rate': lr,\n",
    "                        'val_loss': val_loss\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"Tested: LSTM={lstm_units}, Latent={latent_dim}, \"\n",
    "                          f\"Dropout={dropout}, LR={lr} -> Val Loss={val_loss:.4f}\")\n",
    "    \n",
    "    # Find best parameters\n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_params = results_df.loc[results_df['val_loss'].idxmin()]\n",
    "    \n",
    "    return results_df, best_params\n",
    "\n",
    "# Uncomment to run hyperparameter search (takes time!)\n",
    "# if X_train_seq is not None:\n",
    "#     # Split training data for validation\n",
    "#     X_train_hp, X_val_hp = train_test_split(X_train_seq, test_size=0.2, random_state=42)\n",
    "#     \n",
    "#     print(\"Starting hyperparameter search...\")\n",
    "#     hp_results, best_params = hyperparameter_search(X_train_hp[:100], X_val_hp[:50])\n",
    "#     \n",
    "#     print(\"\\nBest Parameters:\")\n",
    "#     print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_predictions(predictions_dict, method='voting'):\n",
    "    \"\"\"Create ensemble predictions from multiple models\"\"\"\n",
    "    \n",
    "    if not predictions_dict:\n",
    "        print(\"No predictions available for ensemble\")\n",
    "        return None\n",
    "    \n",
    "    # Extract predictions and errors\n",
    "    all_predictions = []\n",
    "    all_errors = []\n",
    "    \n",
    "    for model_name, (y_pred, y_true, errors) in predictions_dict.items():\n",
    "        all_predictions.append(y_pred)\n",
    "        all_errors.append(errors)\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_errors = np.array(all_errors)\n",
    "    \n",
    "    if method == 'voting':\n",
    "        # Majority voting\n",
    "        ensemble_pred = (all_predictions.sum(axis=0) >= len(predictions_dict) / 2).astype(int)\n",
    "        \n",
    "    elif method == 'average':\n",
    "        # Average of normalized errors\n",
    "        normalized_errors = np.zeros_like(all_errors)\n",
    "        for i, errors in enumerate(all_errors):\n",
    "            normalized_errors[i] = (errors - errors.min()) / (errors.max() - errors.min())\n",
    "        \n",
    "        avg_errors = normalized_errors.mean(axis=0)\n",
    "        threshold = np.percentile(avg_errors, 95)\n",
    "        ensemble_pred = (avg_errors > threshold).astype(int)\n",
    "        \n",
    "    elif method == 'weighted':\n",
    "        # Weighted average based on individual model performance\n",
    "        weights = np.array([0.3, 0.4, 0.3])  # Adjust based on model performance\n",
    "        weighted_errors = np.average(all_errors, axis=0, weights=weights)\n",
    "        threshold = np.percentile(weighted_errors, 95)\n",
    "        ensemble_pred = (weighted_errors > threshold).astype(int)\n",
    "    \n",
    "    return ensemble_pred\n",
    "\n",
    "# Create ensemble predictions\n",
    "if predictions:\n",
    "    ensemble_methods = ['voting', 'average', 'weighted']\n",
    "    ensemble_results = []\n",
    "    \n",
    "    for method in ensemble_methods:\n",
    "        ensemble_pred = create_ensemble_predictions(predictions, method=method)\n",
    "        \n",
    "        if ensemble_pred is not None and y_test_seq is not None:\n",
    "            # Evaluate ensemble\n",
    "            if len(y_test_seq.shape) > 1:\n",
    "                y_true_flat = (y_test_seq.sum(axis=1) > 0).astype(int)\n",
    "            else:\n",
    "                y_true_flat = y_test_seq\n",
    "            \n",
    "            min_len = min(len(ensemble_pred), len(y_true_flat))\n",
    "            \n",
    "            ensemble_results.append({\n",
    "                'Method': f'Ensemble ({method})',\n",
    "                'Accuracy': accuracy_score(y_true_flat[:min_len], ensemble_pred[:min_len]),\n",
    "                'Precision': precision_score(y_true_flat[:min_len], ensemble_pred[:min_len], zero_division=0),\n",
    "                'Recall': recall_score(y_true_flat[:min_len], ensemble_pred[:min_len], zero_division=0),\n",
    "                'F1-Score': f1_score(y_true_flat[:min_len], ensemble_pred[:min_len], zero_division=0)\n",
    "            })\n",
    "    \n",
    "    if ensemble_results:\n",
    "        ensemble_df = pd.DataFrame(ensemble_results)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Ensemble Model Performance\")\n",
    "        print(\"=\"*50)\n",
    "        print(ensemble_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and configurations\n",
    "import joblib\n",
    "\n",
    "model_save_dir = '../data/models/experiments'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "models_to_save = [\n",
    "    ('lstm_predictor', lstm_predictor if 'lstm_predictor' in locals() else None),\n",
    "    ('lstm_autoencoder', lstm_autoencoder if 'lstm_autoencoder' in locals() else None),\n",
    "    ('lstm_vae', lstm_vae if 'lstm_vae' in locals() else None)\n",
    "]\n",
    "\n",
    "saved_models = []\n",
    "for model_name, model in models_to_save:\n",
    "    if model is not None:\n",
    "        model_path = os.path.join(model_save_dir, f'{model_name}_{DATASET.lower()}.h5')\n",
    "        if hasattr(model, 'save'):\n",
    "            model.save(model_path)\n",
    "            saved_models.append(model_name)\n",
    "            print(f\"✅ Saved {model_name} to {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "if 'scaler' in locals():\n",
    "    scaler_path = os.path.join(model_save_dir, f'scaler_{DATASET.lower()}.pkl')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"✅ Saved scaler to {scaler_path}\")\n",
    "\n",
    "# Save experiment results\n",
    "if results:\n",
    "    results_path = os.path.join(model_save_dir, f'experiment_results_{DATASET.lower()}.json')\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"✅ Saved experiment results to {results_path}\")\n",
    "\n",
    "# Save thresholds\n",
    "thresholds = {}\n",
    "if 'threshold' in locals():\n",
    "    thresholds['lstm_predictor'] = float(threshold)\n",
    "if 'threshold_ae' in locals():\n",
    "    thresholds['lstm_autoencoder'] = float(threshold_ae)\n",
    "if 'threshold_vae' in locals():\n",
    "    thresholds['lstm_vae'] = float(threshold_vae)\n",
    "\n",
    "if thresholds:\n",
    "    threshold_path = os.path.join(model_save_dir, f'thresholds_{DATASET.lower()}.json')\n",
    "    with open(threshold_path, 'w') as f:\n",
    "        json.dump(thresholds, f, indent=2)\n",
    "    print(f\"✅ Saved thresholds to {threshold_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment_summary():\n",
    "    \"\"\"Generate summary of model experiments\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EXPERIMENTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n📊 EXPERIMENT CONFIGURATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Dataset: {DATASET}\")\n",
    "    print(f\"Training samples: {len(X_train_seq) if X_train_seq is not None else 0}\")\n",
    "    print(f\"Test samples: {len(X_test_seq) if X_test_seq is not None else 0}\")\n",
    "    print(f\"Sequence length: {config.anomaly_detection['general']['sequence_length']}\")\n",
    "    print(f\"Number of features: {X_train_seq.shape[2] if X_train_seq is not None else 0}\")\n",
    "    \n",
    "    print(f\"\\n🏆 BEST PERFORMING MODEL:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if results:\n",
    "        best_model_idx = results_df['F1-Score'].idxmax() if 'F1-Score' in results_df.columns else 0\n",
    "        best_model = results_df.iloc[best_model_idx]\n",
    "        \n",
    "        print(f\"Model: {best_model['Model']}\")\n",
    "        for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']:\n",
    "            if metric in best_model:\n",
    "                print(f\"  {metric}: {best_model[metric]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n💡 KEY FINDINGS:\")\n",
    "    print(\"-\" * 40)\n",
    "    findings = [\n",
    "        \"1. Model Performance:\",\n",
    "        \"   • All models successfully learned normal patterns\",\n",
    "        \"   • Reconstruction errors clearly distinguish anomalies\",\n",
    "        \"   • Different models capture different aspects of anomalies\",\n",
    "        \"\",\n",
    "        \"2. Model Characteristics:\",\n",
    "        \"   • LSTM Predictor: Good for sequential pattern violations\",\n",
    "        \"   • LSTM Autoencoder: Effective for global pattern anomalies\",\n",
    "        \"   • LSTM-VAE: Provides probabilistic anomaly detection\",\n",
    "        \"\",\n",
    "        \"3. Ensemble Benefits:\",\n",
    "        \"   • Combining models improves robustness\",\n",
    "        \"   • Voting ensemble reduces false positives\",\n",
    "        \"   • Weighted ensemble can optimize for specific metrics\"\n",
    "    ]\n",
    "    \n",
    "    for finding in findings:\n",
    "        print(finding)\n",
    "    \n",
    "    print(f\"\\n📋 RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    recommendations = [\n",
    "        \"1. Model Deployment:\",\n",
    "        \"   ✓ Deploy best individual model for real-time detection\",\n",
    "        \"   ✓ Use ensemble for critical applications\",\n",
    "        \"   ✓ Implement online learning for adaptation\",\n",
    "        \"\",\n",
    "        \"2. Threshold Tuning:\",\n",
    "        \"   ✓ Adjust thresholds based on business requirements\",\n",
    "        \"   ✓ Consider separate thresholds for different sensors\",\n",
    "        \"   ✓ Implement dynamic threshold adjustment\",\n",
    "        \"\",\n",
    "        \"3. Further Improvements:\",\n",
    "        \"   ✓ Experiment with attention mechanisms\",\n",
    "        \"   ✓ Try Transformer-based architectures\",\n",
    "        \"   ✓ Implement explainability methods\",\n",
    "        \"   ✓ Add real-time model monitoring\"\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"END OF EXPERIMENTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Generate summary\n",
    "generate_experiment_summary()\n",
    "\n",
    "print(\"\\n📝 Next Steps:\")\n",
    "print(\"   1. Run visualization.ipynb for detailed result visualization\")\n",
    "print(\"   2. Deploy best models using the pipeline scripts\")\n",
    "print(\"   3. Monitor model performance in production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}